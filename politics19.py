# -*- coding: utf-8 -*-
"""politics19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Zlq4C9g8AtgF86KzeYO2UwWKyy0jqXz

InstallRequired Libraries
"""

!pip install pandas numpy scikit-learn wordcloud

"""**Import Necessary Libraries**"""

# Import necessary libraries for data manipulation, visualization, text processing, and building a machine learning model
import pandas as pd
import numpy as np
import string
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from wordcloud import WordCloud
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

"""**Justification:**
This block imports all the required libraries for data manipulation, visualization, text processing, and building machine learning models. These libraries are foundational for carrying out the data mining tasks.

**Load and Preprocess**
"""

# Load the new dataset
file_path = '/content/drive/MyDrive/political_comments_dataset_with_more_columns.xlsx'
new_data = pd.read_excel(file_path)

# Check the first few rows to understand the data
print(new_data.head())

# Check for missing values and fill them if necessary
new_data['comment'] = new_data['comment'].fillna('')

# Convert 'label' to a categorical variable
new_data['label'] = new_data['label'].astype('category')

# Preprocess text by converting to lowercase and removing punctuation
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

new_data['cleaned_comment'] = new_data['comment'].apply(preprocess_text)

"""**Justification:**
This block focuses on data collection and preprocessing, which are critical first steps in data mining. The dataset is loaded, and any missing values in the comment column are filled. The label column is converted to a categorical variable, and text preprocessing is performed to standardize the data by removing punctuation and converting text to lowercase.

**Exploratory Data Analysis (EDA)**
"""

# Visualize the distribution of classes (positive vs negative comments)
sns.countplot(x='label', data=new_data)
plt.title('Distribution of Comment Labels')
plt.show()

# Check and handle empty comments for word clouds
positive_comments = ' '.join(new_data[new_data['label'] == 'positive']['cleaned_comment'])
negative_comments = ' '.join(new_data[new_data['label'] == 'negative']['cleaned_comment'])

if not positive_comments.strip():
    print("No positive comments found.")
if not negative_comments.strip():
    print("No negative comments found.")

plt.figure(figsize=(12, 6))

# Word cloud for positive comments
plt.subplot(1, 2, 1)
if positive_comments.strip():
    positive_wordcloud = WordCloud(width=300, height=200, background_color='white').generate(positive_comments)
    plt.imshow(positive_wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Positive Comments Word Cloud')
else:
    plt.text(0.5, 0.5, 'No Positive Comments', horizontalalignment='center', verticalalignment='center', fontsize=12, color='red')

# Word cloud for negative comments
plt.subplot(1, 2, 2)
if negative_comments.strip():
    negative_wordcloud = WordCloud(width=300, height=200, background_color='white').generate(negative_comments)
    plt.imshow(negative_wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Negative Comments Word Cloud')
else:
    plt.text(0.5, 0.5, 'No Negative Comments', horizontalalignment='center', verticalalignment='center', fontsize=12, color='red')

plt.show()

"""**Justification:**
This block is dedicated to Exploratory Data Analysis (EDA). It visualizes the distribution of the target variable (label) using a count plot. Additionally, word clouds provide insight into the most common words used in positive vs. negative comments, helping to uncover patterns and differences in the text data.

**Feature Extraction Using TF-IDF**
"""

# Define features (X) and target (y)
X_new = new_data['cleaned_comment']
y_new = new_data['label']

# Split data into training and testing sets
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)

# Use TF-IDF to convert text data into numerical features
tfidf = TfidfVectorizer(stop_words='english')
X_train_tfidf_new = tfidf.fit_transform(X_train_new)
X_test_tfidf_new = tfidf.transform(X_test_new)

"""**Justification:**
This block focuses on Feature Extraction using TF-IDF (Term Frequency-Inverse Document Frequency), which is a common technique to convert textual data into numerical features that can be fed into machine learning models. The data is split into training and testing sets, and TF-IDF vectorization transforms the cleaned text data into features.

**Modeling - Random Forest Classifier with Hyperparameter Tuning**
"""

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy')

# Fit GridSearchCV
grid_search.fit(X_train_tfidf_new, y_train_new)

# Get the best model
best_rf_model = grid_search.best_estimator_

# Predict on the test set
y_pred_rf_new = best_rf_model.predict(X_test_tfidf_new)

# Evaluate the model
accuracy_rf_new = accuracy_score(y_test_new, y_pred_rf_new)
print(f"Random Forest Accuracy on New Dataset: {accuracy_rf_new * 100:.2f}%")
print("Classification Report for Random Forest on New Dataset:")
print(classification_report(y_test_new, y_pred_rf_new))

# Confusion matrix
cm_rf_new = confusion_matrix(y_test_new, y_pred_rf_new)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf_new, annot=True, fmt='d', cmap='Blues', xticklabels=new_data['label'].cat.categories, yticklabels=new_data['label'].cat.categories)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Confusion Matrix on New Dataset')
plt.show()

"""**Justification:** This code block performs the entire process of applying the Random Forest classifier to a new dataset, including hyperparameter tuning to find the best model, making predictions, and evaluating the model's performance. The visualizations and metrics help in understanding how well the model performs and where it might need improvements.

**Classical Supervised Machine Learning Algorithms**
"""

# Initialize classifiers with example hyperparameters
models = {
    'Naive Bayes': MultinomialNB(alpha=1.0),
    'Support Vector Machine': SVC(kernel='linear', C=1.0),
    'Decision Tree': DecisionTreeClassifier(max_depth=10, min_samples_split=5, random_state=42),
    'Random Forest': best_rf_model  # Use the best Random Forest model from GridSearchCV
}

# Train and evaluate each model on the new dataset
results_new = {}

for name, model in models.items():
    print(f"Training {name} on new dataset...")
    model.fit(X_train_tfidf_new, y_train_new)
    y_pred_new = model.predict(X_test_tfidf_new)

    # Calculate metrics
    accuracy_new = accuracy_score(y_test_new, y_pred_new)
    report_new = classification_report(y_test_new, y_pred_new)
    cm_new = confusion_matrix(y_test_new, y_pred_new)

    # Store results
    results_new[name] = {
        'accuracy': accuracy_new,
        'classification_report': report_new,
        'confusion_matrix': cm_new
    }

    # Print results
    print(f"{name} Accuracy on new dataset: {accuracy_new * 100:.2f}%")
    print("Classification Report on new dataset:")
    print(report_new)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_new, annot=True, fmt='d', cmap='Blues', xticklabels=new_data['label'].cat.categories, yticklabels=new_data['label'].cat.categories)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'{name} Confusion Matrix on New Dataset')
    plt.show()

"""**Justification**: This block of code systematically trains and evaluates multiple classification models, including a previously optimized Random Forest model, on the new dataset. By comparing accuracy, detailed classification metrics, and confusion matrices, it provides a comprehensive view of each model's performance. This approach helps in selecting the best model based on empirical results and understanding its strengths and weaknesses.

**Neural Network Classification Using TensorFlow/Keras**
"""

# Step 1: Convert categorical labels to integer labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train_new)
y_test_encoded = label_encoder.transform(y_test_new)

# Step 2: Convert integer labels to categorical format
y_train_cat_new = to_categorical(y_train_encoded)
y_test_cat_new = to_categorical(y_test_encoded)

# Step 3: Define the neural network model
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(X_train_tfidf_new.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(y_train_cat_new.shape[1], activation='softmax'))  # Output layer with softmax activation

# Step 4: Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(X_train_tfidf_new.toarray(), y_train_cat_new,
                    epochs=10,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=1)

# Step 6: Evaluate the model
loss, accuracy = model.evaluate(X_test_tfidf_new.toarray(), y_test_cat_new, verbose=1)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Step 7: Predict and evaluate
y_pred = model.predict(X_test_tfidf_new.toarray())
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test_cat_new, axis=1)

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))

# Plot training & validation accuracy values
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""**Justification:**
This block encodes the target labels using LabelEncoder, which is crucial for transforming categorical labels into numerical form for model training. A neural network model is then defined, compiled, and trained on the TF-IDF features. The use of to_categorical ensures the labels are in a format compatible with the neural network's output.

**Making Predictions**
"""

def predict_comment(comment, models, tfidf, nn_model=None):
    # Preprocess and transform the comment
    cleaned_comment = preprocess_text(comment)
    comment_tfidf = tfidf.transform([cleaned_comment])

    predictions = {}
    for name, model in models.items():
        y_pred = model.predict(comment_tfidf)
        prediction = 'User' if y_pred[0] == 0 else 'Bot'
        predictions[name] = prediction

    if nn_model:
        y_pred = nn_model.predict(comment_tfidf.toarray())

        # Debugging: Print type and content of y_pred
        print(f"Type of y_pred: {type(y_pred)}")
        print(f"Content of y_pred: {y_pred}")

        # Handle case where y_pred contains class labels
        if isinstance(y_pred, np.ndarray):
            if y_pred.ndim == 1 and isinstance(y_pred[0], (int, float)):
                # Binary classification with sigmoid output
                prediction = 'User' if y_pred[0] > 0.5 else 'Bot'
            else:
                # Multi-class classification with softmax output or if y_pred is already class labels
                y_pred_classes = np.argmax(y_pred, axis=1) if y_pred.ndim > 1 else y_pred
                prediction = 'User' if y_pred_classes[0] == 0 else 'Bot'
            predictions['Neural Network'] = prediction
        else:
            print("Neural network output is not a numeric array. Please check the model.")

    return predictions

# Interactive CLI example
comment_input = input("Enter a comment to classify: ")
predictions = predict_comment(comment_input, models, tfidf, model)
print("Predictions:")
for name, prediction in predictions.items():
    print(f"{name} Prediction: {prediction}")